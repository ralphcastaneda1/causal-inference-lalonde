{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691dbb79",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab09.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af45b367-3f10-4ed2-8fe4-891d760a443b",
   "metadata": {},
   "source": [
    "# Lab 9: Unconfoundedness\n",
    "\n",
    "Welcome to the 9th DS102 lab! \n",
    "\n",
    "In this lab, you will explore the challenges of doing causal inference without randomization using the unconfoundedness assumption. We will help you reproduce results from a real labor economics application in a very famous paper by Robert Lalonde. Hopefully this will give you a taste of the issues that come up in practice. \n",
    "\n",
    "The code you need to write is indicated by `...`\n",
    "\n",
    "## Collaboration Policy\n",
    "You can submit the lab in pairs (groups of two, no more than two). **If you choose to work in a pair, please make sure to add your group member on Gradescope for both written and code submission.**\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually and do not share your code with anyone other than your partner**. If you do discuss the assignments with people other than your partner please **include their names** in the cell below.\n",
    "\n",
    "`<Collaborator Name> <Collaborator e-mail>`\n",
    "\n",
    "## Submission\n",
    "**For full credit, this assignment should be completed and submitted before Wednesday, Apr 2nd, 2025 at 05:00 PM PST.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0980b4ed-5166-40ab-934f-7a4d05cd5308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "\n",
    "import hashlib\n",
    "def get_hash(num, significance = 4):\n",
    "    num = round(num, significance)\n",
    "    \"\"\"Helper function for assessing correctness\"\"\"\n",
    "    return hashlib.md5(str(num).encode()).hexdigest()\n",
    "\n",
    "sns.set(style=\"dark\")\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4e8416-df4d-4622-8ec4-aa0b9bf5a65f",
   "metadata": {},
   "source": [
    "# Causal Inference Background and Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b698b-c0c4-43ed-aaaf-6de9cb538525",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Potential Outcomes and Average Treatment Effect\n",
    "\n",
    "In general, we can measure the causal effect of a binary treatment $Z$ on an outcome $Y$ by considering the potential outcomes $Y(0)$ and $Y(1)$. Recall that these are *potential* outcomes: they represent thought experiments about what would happen if the treatment was or wasn't applied. In the real world, we only ever get to observe one of them for any individual, depending on whether that unit received the treatment or not.\n",
    "\n",
    "We defined the average treatment effect (ATE), represented by the Greek letter tau ($\\tau$), as:\n",
    "\n",
    "$$\n",
    "\\tau = E[Y(1) - Y(0)]\n",
    "$$\n",
    "\n",
    "This represents the causal effect of a treatment $Z$ on an outcome $Y$. We saw that in general, we were unable to estimate this without making assumptions. In the special case where our data come from a randomized experiment, then we saw that the Simple Difference in Observed group means (SDO) was an unbiased estimate of the ATE:\n",
    "\n",
    "$$\n",
    "\\hat{\\tau} = \\underbrace{\\frac{1}{n_1}\\sum_{i: Z_i = 1} Y_i}_{\\text{mean of treatment group}} -  \\underbrace{\\frac{1}{n_0}\\sum_{i: Z_i = 0} Y_i}_{\\text{mean of control group}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a85a95d-b09c-434c-bbce-4fa86e028232",
   "metadata": {},
   "source": [
    "## Independence and Unconfoundedness\n",
    "Recall that in a randomized experiment, we make treatment decisions completely at random. This prevents the treatment from being confounded by any external factors - if it was assigned completely at random then treatment cannot be correlated with unobserved variables. Unfortunately, in an observational study, we often must deal with confounders: variables that are correlated with both the treatment and the outcome. Mathematically, in a randomized experiment, we say that\n",
    "\n",
    "$$\n",
    "Z_i \\perp\\!\\!\\!\\perp \\big(Y_i(0), Y_i(1)\\big)\\,\\, \\forall i\n",
    "$$\n",
    "\n",
    "meaning that knowing any unit's treatment status doesn't give us any additional information about the distribution of their potential outcomes (\"what-ifs\"). For example, in a drug trial, because of randomization, the people who receive the drug have the same (distribution of) potential outcomes as the people who receive a placebo, since there are no systematic differences between the treatment and control groups.\n",
    "\n",
    "In an observational study, this usually isn't true. For example, suppose we are interested in the effect of a job training program on income. People who receive the job training program might be poorer than people who don't, and so whether they receive the training or not, their incomes might be lower. Therefore, the confounding effect of socioeconomic status will cause the simple difference in means to be biased. If we observe socioeconomic status, then we can use various strategies to correct for it to recover the true causal effect of job training programs. \n",
    "\n",
    "This idea, that we observe all confounding varaibles so we can correct for them, is called **unconfoundedness**. Stated mathematically, the treatment and potential outcomes are *conditionally* independent given a set of known confounding variables $X$:\n",
    "\n",
    "$$\n",
    "Z_i \\perp\\!\\!\\!\\perp \\big(Y_i(0), Y_i(1)\\big) \\mid X_i \\,\\, \\forall i\n",
    "$$\n",
    "\n",
    "If we make this assumption, we can use a few different approaches to estimate the average treatment effect. \n",
    "\n",
    "**Note:** work on causal inference has happened across many different fields - medicine, economics, education, psychology, etc. Unfortunately, in different fields, the unconfoundedness assumption goes by many different names. Some other popular names for unconfoundedness include: \"exogeneity\" (economics), \"selection on observables\" (statistics), and \"ignorability\" (medicine)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde940fa-07ee-4cb6-afde-8b788ca13f9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Problem Setup and Data\n",
    "\n",
    "For this lab, you'll be working with data from a job training program in the mid-1970s called the National Supported Work Demonstration. The data (and the results we'll reproduce) come from two famous papers. The first is a 1986 paper by Robert LaLonde, [Evaluating the Econometric Evaluations of Training Programs](https://business.baylor.edu/scott_cunningham/teaching/lalonde-1986.pdf). The second is a follow-up paper by Rajeev Dehejia and Sadek Wahba, [Propensity Score Matching Methods for Non-experimental Causal Studies](https://www.nber.org/papers/w6829), which as you can probably guess, applied propensity score methods. \n",
    "\n",
    "Here's a description of the NSW program from the original LaLonde paper (emphasis added):\n",
    "\n",
    "> The National Supported Work Demonstration (NSW) was a temporary employment program designed to help disadvantaged workers lacking basic job skills move into the labor market by giving them work experience and counseling in a sheltered environment. Unlike other federally sponsored employment and training programs, the NSW program **assigned qualified applicants to training positions randomly**. Those assigned to the treatment group received all the benefits of the NSW program, while those assigned to the control group were left to fend for themselves.\n",
    "\n",
    "Here are a few more important excerpts from the paper, describing the participants and data collected (links added):\n",
    "\n",
    "> The MDRC admitted into the program [AFDC women](https://en.wikipedia.org/wiki/Aid_to_Families_with_Dependent_Children), ex-drug addicts, ex-criminal offenders, and high school dropouts of both sexes. For those assigned to the treatment group, the program guaranteed a job for 9 to 18 months, depending on the target group and site. The treatment group was divided into crews of three to five participants who worked together and met frequently with an NSW counselor to discuss grievances and performance...\n",
    "\n",
    "> The type of work even varied within sites. In particular, male and female participants frequently performed different sorts of work. The female participants usually worked in service occupations, whereas the male participants tended to work in construction occupations.\n",
    "\n",
    "> The MDRC collected earnings and demographic information from both the treatment and the control group at baseline and every nine months thereafter. MDRC also conducted up to four post-baseline interviews.\n",
    "\n",
    "Our goal will be to estimate the causal effect of the training program on income. Specifically, we will compare the income of people in 1974, 1975 (before the training program) with their income in 1978 (after the program).\n",
    "\n",
    "Just like LaLonde did, we'll start by evaluating the randomized experiment. Then, we'll look at what would happen if we didn't have a control group, and instead had to use data from an observational study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f86899-bf38-45cd-97f1-8ca727018791",
   "metadata": {},
   "source": [
    "# Part I: Randomized Experiment\n",
    "\n",
    "Let's begin by looking at the data from the NSW experiment. We provide you with a subset of the original data studied in the Dehejia & Wahba paper, which is commonly used as a benchmark in causal inference. Note that this subset **only includes male participants and only those participants whose income in both 1974 and 1975 are observed** which will simplify the analysis.\n",
    "\n",
    "It contains the following columns:\n",
    "* `data_id`: always `NSW`, indicating that the data are from the NSW randomized experiment\n",
    "* `treat`: binary variable indicating treatment (1 for job training, 0 for control)\n",
    "* `age`: age in years\n",
    "* `educ`: number of years of education\n",
    "* `black`: whether the worker was Black (1) or not (0).\n",
    "* `hisp`: whether the worker was Hispanic (1) or not (0).\n",
    "* `marr`: whether the worker was married (1) or not (0).\n",
    "* `nodegree`: whether the worker had a high school diploma (0) or not (1).\n",
    "* `re74`, `re75`: earnings in 1974 and 1975, before the program\n",
    "* `re78`: earnings in 1978, after the program.\n",
    "* `outcome`: difference in earnings from 1974 to 1978"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be394db-8176-4f0c-b3c5-e1497569101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsw = pd.read_csv('nsw_dw.csv')\n",
    "nsw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b144d8-9b2a-42a1-aa71-1ac46a1c9f4a",
   "metadata": {},
   "source": [
    "In Part I, we leverage the fact that the participants were randomly assigned to the treatment group, i.e attending the training program (`treat = 1`) and the control group, i.e. not attending the training program (`treat = 0`). Hence, we can compute the causal effect using the following expression:\n",
    "\n",
    "$$\n",
    "\\hat{\\tau} = \\underbrace{\\frac{1}{n_1}\\sum_{i: Z_i = 1} Y_i}_{\\text{mean of treatment group}} -  \\underbrace{\\frac{1}{n_0}\\sum_{i: Z_i = 0} Y_i}_{\\text{mean of control group}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafdd343",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 1.a) Understanding causal inference assumptions\n",
    "\n",
    "Even though the NSW program had male and female participants, our dataset only contains the men. Consider this paragraph from the original LaLonde paper:\n",
    "\n",
    "> In particular, male and female participants frequently performed different sorts of work. The female participants usually worked in service occupations, whereas the male participants tended to work in construction occupations.\n",
    "\n",
    "If we were to include the data from men and women and analyze them together, which assumption(s) would we be violating and why?\n",
    "\n",
    "*Hint: remember that this is a randomized experiment.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f447c6dc",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "enter answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d271e06f-2bf7-498e-88f5-7cacef79fcaf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 1.b)  Compute causal effect in randomized experiments\n",
    "\n",
    "Complete the code below to output the causal effect of training program on participants' income using the expression above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b277dc-8853-45a8-9318-3ec11fa766fe",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "causal_effect_nsw =...\n",
    "causal_effect_nsw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc8ad1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e250c7dc-39ee-4070-8dd4-76a4a442844e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 1.c) Interpret the result\n",
    "Based on your answer above, what is the causal effect of attending the training program on income? In other words, does attending the training program lead to higher income?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7965bd",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "enter answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf691f86-ded8-453e-9008-706e4dbd8411",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Part II: Using an Observational Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b92db1-424b-408a-8ce0-83c4c7648011",
   "metadata": {},
   "source": [
    "Often, we want to estimate the causal effect of a treatment without randomization. One issue is that because we can never observe the counterfactual potential outcomes (\"the fundamental problem of causal inference\"), in a typical observational study we can never check if our answer is correct. So then can we ever be sure that observational causal inference tools really work on real data? One of the reasons Lalonde's paper is so famous is because he had a brilliant idea about how to test observational methods. \n",
    "\n",
    "Since NSW was randomized, we know that we have an unbiased estimate of the true average treatment effect. Lalonde's idea was to replace the randomized control group with non-random observational data. Then one could apply causal inference methods to the true treated group and the observational control group like one would do in a normal observational study. But this time, **we know what the average treatment effect is supposed to be**, so we can check our answer afterward!\n",
    "\n",
    "LaLonde used the Current Population Survey (CPS), a publicly available dataset, as a control group. Let's now look at this data: for your convenience, it has the same columns as the NSW data above. Note that it's much larger!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d92dab-8749-427d-a7c6-f458e93a5972",
   "metadata": {},
   "outputs": [],
   "source": [
    "cps = pd.read_csv('cps.csv')\n",
    "cps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd179d8-f663-40c6-9df4-526affba36d5",
   "metadata": {},
   "source": [
    "For the rest of the lab, we'll work with a modified version of the data that doesn't have any randomized controls, only the ones from the general population. In the cell below, we creat a new dataframe called `obs` by concatenating the `cps` dataframe with rows of the `nsw` dataframe corresponding to the people who attended the training program.\n",
    "\n",
    "**Your answers to all remaining questions should only use the `obs` table, not the `nsw` table!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d91e710-a60d-48b3-8d45-0e263d1b00d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hisp</th>\n",
       "      <th>marr</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSW</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9930.0460</td>\n",
       "      <td>9930.0460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NSW</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3595.8940</td>\n",
       "      <td>3595.8940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NSW</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24909.4500</td>\n",
       "      <td>24909.4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NSW</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7506.1460</td>\n",
       "      <td>7506.1460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NSW</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>289.7899</td>\n",
       "      <td>289.7899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16172</th>\n",
       "      <td>CPS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3975.352</td>\n",
       "      <td>6801.435</td>\n",
       "      <td>2757.4380</td>\n",
       "      <td>-1217.9141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16173</th>\n",
       "      <td>CPS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1445.939</td>\n",
       "      <td>11832.240</td>\n",
       "      <td>6895.0720</td>\n",
       "      <td>5449.1330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16174</th>\n",
       "      <td>CPS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1733.951</td>\n",
       "      <td>1559.371</td>\n",
       "      <td>4221.8650</td>\n",
       "      <td>2487.9140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16175</th>\n",
       "      <td>CPS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16914.350</td>\n",
       "      <td>11384.660</td>\n",
       "      <td>13671.9300</td>\n",
       "      <td>-3242.4200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16176</th>\n",
       "      <td>CPS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13628.660</td>\n",
       "      <td>13144.550</td>\n",
       "      <td>7979.7240</td>\n",
       "      <td>-5648.9360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16177 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      data_id  treat   age  educ  black  hisp  marr  nodegree       re74  \\\n",
       "0         NSW    1.0  37.0  11.0    1.0   0.0   1.0       1.0      0.000   \n",
       "1         NSW    1.0  22.0   9.0    0.0   1.0   0.0       1.0      0.000   \n",
       "2         NSW    1.0  30.0  12.0    1.0   0.0   0.0       0.0      0.000   \n",
       "3         NSW    1.0  27.0  11.0    1.0   0.0   0.0       1.0      0.000   \n",
       "4         NSW    1.0  33.0   8.0    1.0   0.0   0.0       1.0      0.000   \n",
       "...       ...    ...   ...   ...    ...   ...   ...       ...        ...   \n",
       "16172     CPS    0.0  22.0  12.0    1.0   0.0   0.0       0.0   3975.352   \n",
       "16173     CPS    0.0  20.0  12.0    1.0   0.0   1.0       0.0   1445.939   \n",
       "16174     CPS    0.0  37.0  12.0    0.0   0.0   0.0       0.0   1733.951   \n",
       "16175     CPS    0.0  47.0   9.0    0.0   0.0   1.0       1.0  16914.350   \n",
       "16176     CPS    0.0  40.0  10.0    0.0   0.0   0.0       1.0  13628.660   \n",
       "\n",
       "            re75        re78     outcome  \n",
       "0          0.000   9930.0460   9930.0460  \n",
       "1          0.000   3595.8940   3595.8940  \n",
       "2          0.000  24909.4500  24909.4500  \n",
       "3          0.000   7506.1460   7506.1460  \n",
       "4          0.000    289.7899    289.7899  \n",
       "...          ...         ...         ...  \n",
       "16172   6801.435   2757.4380  -1217.9141  \n",
       "16173  11832.240   6895.0720   5449.1330  \n",
       "16174   1559.371   4221.8650   2487.9140  \n",
       "16175  11384.660  13671.9300  -3242.4200  \n",
       "16176  13144.550   7979.7240  -5648.9360  \n",
       "\n",
       "[16177 rows x 12 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treated = nsw[nsw['treat'] == 1]\n",
    "obs = pd.concat([treated, cps], ignore_index=True)\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44b07f8-8434-4abb-be00-2253c1037751",
   "metadata": {},
   "source": [
    "The following histogram compares the distribution of education between the NSW treatment group and the CPS group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd81982-00be-4cd2-8971-e9cdd371c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=obs, x='educ', hue='data_id', stat='density', binwidth=0.5, common_norm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b8c89b-f804-416d-a5ae-c87f66842f7d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 2.a) Evaluating a possible confounder \n",
    "\n",
    "Based on the histogram above, we can say that education is a confounding variable. How can you justify this claim? In other words, why is education a confounding variable?\n",
    "\n",
    "*Hint: What kind of association do you expect between education and income?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730bfdd",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "enter answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7c6f74-81b9-45d8-91b4-7c3c10b76f36",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 2.b) Computing the SDO\n",
    "As our first attempt to estimate the causal effect, we decide to try what we did in Question 1. In other words, we compute the Simple Difference in Observed group means (SDO) for this observational data. \n",
    "\n",
    "Complete the code below to output compute the SDO using dataset `obs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a7dd9-b14e-488d-aa7c-7e97311fb794",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "sdo ...\n",
    "sdo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac78f0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73397a90-0128-49f9-85d5-8ad8d55aa450",
   "metadata": {},
   "source": [
    "The result that you found should be roughly $1600 larger than that of the causal effect that we measured in the randomized experiment. This is because of confounding: even though the actual effect of the program on income is much smaller (as we saw from the randomized experiment), the treatment group and our CPS group are very different. In particular, individuals in the treatment group faced many disadvantages that cause their earnings to be lower initially, and also caused them to be more likely to end up in the treatment group. For example, the NSW sample has systemtically lower levels of education than the CPS sample as we saw in Question 2a. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6482e714-07f9-4e05-92a2-25d3038d7dcf",
   "metadata": {},
   "source": [
    "# Part III: Unconfoundedness Techniques\n",
    "Now let's go ahead and implement the techniques we discussed in class that utilize unconfoundedness to estimate the treatment effect.\n",
    "\n",
    "**REMINDER:** the unconfoundedness assumption means we observe all the relevant confounding variables (like years of education). I.e. there are no **unobserved** confounders.\n",
    "\n",
    "**Note**: For those of you who are interested in reading more, the setting that we're discussing here is often called a **Difference-in-Differences (DID)** experimental approach. It simply describes the usage of causal inference techniques when the treatment effect we're trying to estimate is being represented as a difference (i.e between those who did and didn't go through the program) in differences (i.e how income changed for each individual between 1978 and income in 1974)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51708be5-7ca2-4e2a-a7e9-55ff0659bf41",
   "metadata": {},
   "source": [
    "## Technique 1. Outcome Regression\n",
    "We know that education is a confounder. If we knew that education were the **only** confounder, then by unconfoundedness, we could fit a linear model of the following form:\n",
    "\n",
    "$\\text{Earnings} = \\tau * Z + b*\\text{years of education}$\n",
    "\n",
    "We saw in lecture that if we make two assumptions, then the estimated coefficient of treament from OLS, $\\hat{\\tau}$, will be an unbiased estimate of the ATE. The two assumptions are:\n",
    "\n",
    "1. Assume unconfoundedness given education.\n",
    "2. Assume this linear model correctly describes the interaction between the variables.\n",
    "\n",
    "Let's try it and see how well it performs. The code below are taken from previous labs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad08b37-fa8e-4fce-a4d6-c153745a553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here: Just examine the code\n",
    "def fit_OLS_model(df, target_variable, explanatory_variables, intercept = False):\n",
    "    \"\"\"\n",
    "    Fits an OLS model from data.\n",
    "    \n",
    "    Inputs:\n",
    "        df: pandas DataFrame\n",
    "        target_variable: string, name of the target variable\n",
    "        explanatory_variables: list of strings, names of the explanatory variables\n",
    "        intercept: bool, if True add intercept term\n",
    "    Outputs:\n",
    "        fitted_model: model containing OLS regression results\n",
    "    \"\"\"\n",
    "    \n",
    "    target = df[target_variable]\n",
    "    inputs = df[explanatory_variables]\n",
    "    if intercept:\n",
    "        inputs = sm.add_constant(inputs)\n",
    "    \n",
    "    fitted_model = sm.OLS(target, inputs).fit()\n",
    "    return(fitted_model)\n",
    "\n",
    "def mean_squared_error(true_vals, predicted_vals):\n",
    "    \"\"\"\n",
    "    Return the mean squared error\n",
    "    \n",
    "    Inputs:\n",
    "        true_vals: array of true labels\n",
    "        predicted_vals: array labels predicted from the data\n",
    "    Output:\n",
    "        float, mean squared error of the predicted values\n",
    "    \"\"\"\n",
    "    return np.mean((true_vals - predicted_vals) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac49b1-7d6d-45b5-9885-dbb64581d653",
   "metadata": {},
   "source": [
    "As a reminder, in previous labs, we used it like this: `fit_OLS_model(student_data, 'NumBooks', ['ReadathonDuration', 'Income'])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c969ed-b844-4b4f-9b83-db4b0b329330",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 3.a) Fitting our model (single confounder)\n",
    "\n",
    "Complete the code below by using the functions above to fit the linear model described above to predict the difference in earnings from 1974 to 1978 (i.e the `outcome` column) from the treatment and the education variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad1cc6e-2d0d-4244-bb88-13008a794718",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "linear_model =  ...\n",
    "print(linear_model.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e2e10b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8124d938-84e1-4546-abdb-8541225aca0c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 3.b) Evaluating our model (single confounder)\n",
    "\n",
    "Was our estimate of the treatment effect right or wrong? Roughly how far off were we from the truth? Can you think of a simple reason why the answer is so wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58ab819",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "enter answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9c5d4b-956f-4266-aa80-a523093ac76c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Adding More Controls\n",
    "Now, suppose the full set of variables (age, years of education, Black/Hispanic race, marriage, and diploma) are the only confounders in this problem. In that case, we can make the unconfoundedness assumption, where $X$ represents the collection of all 6 confounding variables listed above. **We will make this assumption for the rest of the lab.**\n",
    "\n",
    "Suppose we fit a linear model of the following form:\n",
    "\n",
    "$\\text{Earnings} = \\tau * Z + a*\\text{age} + b*\\text{years of education} + c*\\text{isBlack} + d*\\text{isHispanic} + e*\\text{isMarried} + f*\\text{hasDiploma}$\n",
    "\n",
    "Again, under the following assumptions, the estimated coefficient of treament from OLS, $\\hat{\\tau}$, will be an unbiased estimate of the ATE:\n",
    "\n",
    "1. Assume unconfoundedness given this set of 6 variables.\n",
    "2. Assume this new linear model correctly describes the interaction between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3582f040-f895-44f2-aafc-57cb67ced583",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 3.c) Fitting our model (multiple confounders)\n",
    "\n",
    "Complete the code below by using the functions above to fit the linear model described above to predict the difference in earnings from 1974 to 1978 (i.e the `outcome` column) from the treatment and the 6 confounding variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba063e3-e3d1-499d-9aa7-af8b4918d2ad",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "full_linear_model = ...\n",
    "print(full_linear_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba11b1ef",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a930e9c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 3.d) Evaluating our model using only observational data information\n",
    "\n",
    "Suppose we did not have access to the true causal effect that we computed in Part 1. Using what you've learned in Data 102, how could you compare the two models in parts 3.a) and 3.c)? According to your answer, which model is a better fit and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d20200a",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "enter answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cb24a1-07a7-47e0-b181-27469426157d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 3.e) Evaluating our model (multiple confounders)\n",
    "\n",
    "Now, let's use the fact that we know the true answer from Part 1. Was our estimate of the treatment effect from 3.c right or wrong? Is our answer better or worse than the answer from the simpler model in 3.a? Explain whether this is consistent with your answer from 3.c and why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9f1c49",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "Our answer from 3c was closer to the true effect from the randomized experiment, where we had an outcome of around $1800. So adding more predictors got us closer to the true outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e82a12-2803-4416-be94-d5d9ca981730",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "As mentioned, we'll take assumption 1 for granted for now. Assumption 2, however, is much more questionable: **it's not clear that our model correctly describes the interaction between the variables**. In particular, it assumes that the effect of each confounder is the same for both treatment and control, an assumption that is often violated.\n",
    "\n",
    "\n",
    "For example, married individuals in the CPS sample might have more financial stability (since they may wait for financial stability to get married), which might not be true in the NSW sample (where individuals have much lower financial stability overall). But, the model above only uses one coefficient, $e$, for the effect of marriage on income, regardless of whether an individual is from the treatment or control.\n",
    "\n",
    "For example, consider the following two histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5d0581-5ada-482b-b939-af8f78c594d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=obs[obs['data_id']=='NSW'], x='age', hue='marr', stat='density', common_norm=False, binwidth=2);\n",
    "plt.title(\"Distributions of age under different marriage status in NSW group\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debe45d1-f62f-418d-b9dc-27d29a5ca957",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=obs[obs['data_id']=='CPS'], x='age', hue='marr', stat='density', common_norm=False, binwidth=2);\n",
    "plt.title(\"Distributions of age under different marriage status in CPS group\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afc45fe-1126-40f2-aa2c-b3e4c584f3c5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 3.f) Identifying model misspecification\n",
    "\n",
    "Use the two histograms to argue why the simple linear model might be misspecified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87402ccd",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "enter answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc7fe7a-674b-4f97-86c3-46c9c1fb6c27",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Frequentist Perspectives on Causal Inference\n",
    "\n",
    "Now, let's imagine that we had correctly specified our linear model, and that both assumption 1 and assumption 2 magically hold. Even then, there is still a chance that our estimated treatment effect $\\hat{\\tau}$ won't exactly line up with the true treatment effect! Why might this be the case?\n",
    "\n",
    "To answer this question, consider the lens through which we are viewing causal inference: looking through this lab, you'll notice that our discussion centers around ***estimated*** treatment effects $\\hat{\\tau}$ that approximate some ***true, fixed, hidden*** average treatment effect $\\tau$. This language should signal to you that we're utilizing a frequentist approach to causal inference.\n",
    "\n",
    "If we're applying those same principles of frequentism here, the reason why our estimated treatment effect $\\hat{\\tau}$ won't always perfectly estimate the true treatment effect (despite meeting all our assumptions) is because of **randomness in the data**. Depending on the observational data that we have, we will calculate different estimates of the average treatment effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74ce7e2-8da8-406e-8d79-c364da988fe3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 3.g) Bootstrapped estimates of the ATE\n",
    "\n",
    "Continuing in this train of thought, one way to account for this uncertainty in our analysis is to create an interval that will, with 95% confidence, contain the true average treatment effect. To do this, we can employ a strategy that we explored earlier in the course: the bootstrap!\n",
    "\n",
    "Fill in the skeleton code below to create a bootstrap algorithm. We'll use this function to compute the 95% confidence interval on the estimates of $\\tau$.\n",
    "\n",
    "*Hint: To pass the autograder, make sure you sample the data using `pandas` (see the [documentation here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html)) and pass the `random_state` argument to the function call.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b98f8f7-2a31-496c-892b-a9424091cc48",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#TODO: Fill in the missing values\n",
    "def draw_tau_hat(data=obs, random_state=None):\n",
    "    \"\"\"Returns a single bootstrapped estimate of the ATE.\n",
    "    \n",
    "    Inputs:\n",
    "        data: DataFrame of observed data\n",
    "        random_state: integer to help standardize sampling output \n",
    "        for grading and troubleshooting purposes\n",
    "    Output:\n",
    "        float, estimated average treatment effect\n",
    "    \"\"\"\n",
    "    sample = ...\n",
    "    fit_model = ...\n",
    "    estimated_ate = ...\n",
    "    return estimated_ate\n",
    "\n",
    "def get_bootstrapped_ate(data=obs, n=1000):\n",
    "    \"\"\"Returns n bootstrapped estimates of the ATE.\n",
    "    \n",
    "    Inputs:\n",
    "        data: DataFrame of observed data\n",
    "        n: number of bootstrapped estimates we would like to return\n",
    "    Output:\n",
    "        list of estimated average treatment effects\n",
    "    \"\"\"\n",
    "    estimates = []\n",
    "    for i in np.arange(n):\n",
    "        ...\n",
    "    return estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e0c55d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc44b645-d081-43cc-8618-ea83b493d41f",
   "metadata": {},
   "source": [
    "Now that we've defined a few functions, run the code below to calculate our confidence interval and visualize our sampling distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97796f6-a661-4e04-bd41-2dc5d9a84459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here: just run the code!\n",
    "ates = get_bootstrapped_ate(obs, 1000)\n",
    "confidence_interval = [np.percentile(ates, 2.5), \n",
    "                       np.percentile(ates, 97.5)]\n",
    "print(f\"Our 95% confidence interval ranges from {np.round(confidence_interval[0])} to {np.round(confidence_interval[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a30ba61-bac3-4986-ad4e-f4d4bf736c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(ates)\n",
    "plt.hlines(1, confidence_interval[0], confidence_interval[1], linewidth=5)\n",
    "plt.title(\"Bootstrapped Estimates of the ATE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98db710-6c8f-4a8f-af38-46bf77549420",
   "metadata": {},
   "source": [
    "If you've defined your functions correctly, you should notice that our 95% confidence interval indeed captures our true ATE that we calculated at the start of the lab, and also does not include zero, making our result statistically significant!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6be750c-6307-404d-aaed-d3860276b454",
   "metadata": {},
   "source": [
    "## Technique 2: Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a71ef1-3a85-4569-a6b1-7ba5f7458eee",
   "metadata": {},
   "source": [
    "We have seen above that a simple linear regression model is not ideal - even if we add all of our variables as controls. Lalonde used these findings to argue that linear regression for causal inference is highly unreliable. Now, we consider a different technique introduced in lecture called matching. \n",
    "\n",
    "Consider two individuals, one treated and one untreated, with the exact same values of all confounding variables $X$. Here's an example of someone from the NSW study and someone from the CPS data with the exact same set of confounding variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cf2f57-4263-4502-92f6-212083e0c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsw.iloc[50:51, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97f4707-94e5-4dda-920a-b14fd607af3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cps.iloc[2363:2364, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8eb479-8e65-4610-b66c-2c832520a50f",
   "metadata": {},
   "source": [
    "If we assume unconfoundedness, then for these two people, there should be no other variables that have an effect on both the treatment and the outcome. So, by subtracting their outcomes, we should be able to estimate the causal effect of the job training program for this particular $X$ (specifically, 28-year old, unmarried, Black, non-Hispanic people without high school diplomas who've completed 8 years of schooling).\n",
    "\n",
    "If we do this for every possible set of values for the confounders $X$, then we can take all of them and compute the expectation (weighting each by the probability of seeing that corresponding value of $X$). Empirically, this corresponds to just taking the average of all the data points.\n",
    "\n",
    "Here is the matching algorithm in English:\n",
    "\n",
    "1. For each treated row:\n",
    "\n",
    "   * Find all untreated rows that have the exact same values of all confounders.\n",
    "   * Take those untreated rows and average their outcome\n",
    "   * Subtract the average above from the treated row's outcome\n",
    "\n",
    "\n",
    "2. For each *untreated* row:\n",
    "   * Find all *treated* rows that have the exact same values of all confounders.\n",
    "   * Take those *treated* rows and average their outcome\n",
    "   * Subtract the *untreated* row's outcome from the average above\n",
    "\n",
    "3. Average all the results from steps 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffbc88b-93d0-4493-9e48-3d4ffc1017f3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 4.a) Challenges with exact matching\n",
    "Explain why this exact maching algorithm will not work for the dataset provided. \n",
    "\n",
    "*Hint: There are six whole variables for every observation that we would need to find an exact match for.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d9794d",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "enter answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16378868-7f20-4f63-987d-c2a8737ff9d3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "There are solutions such as approximate matching which matches people if they have similar features (not necessarily identical), but we'll instead turn to using propensity scores instead. Propensity scores are a dimensionality-reduction technique, that map all 6 confounders down to a single value per observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aefd1c3-d7fd-4519-813e-a5e04bb81556",
   "metadata": {},
   "source": [
    "## Technique 3: Inverse Propensity Weighting\n",
    "We've already seen that for this dataset, we can't use the simple difference in observed group means (SDO) to estimate the causal ATE. In this section, we'll try inverse propensity weighting instead. Recall the definition of the propensity score: it is the probability that a unit was treated, conditioned on a particular set of confounders $x$:\n",
    "\n",
    "$$\n",
    "e(x) = P(Z=1 | X=x)\n",
    "$$\n",
    "\n",
    "This is also sometimes called a \"selection model\" because it uses the covariates to model how observations are selected into treatment or control.  The simplest and most common way to compute propensity scores is using logistic regression: you'll get practice with this on HW4. In particular, in this example, we would use the `treat` column as our target variable and the confounders as our predictors.\n",
    "\n",
    "In this lab, we have computed the propensity scores for you using a slightly more complex model that includes income before the program (`re74`) and flexibly models some nonlinear interactions. You'll get some practice with computing propensity scores on HW4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fffdfed-786b-45d2-ad34-c75a3c761b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import obs data with propensity scores computed\n",
    "obs_prop = pd.read_csv('obs_with_propensity_scores.csv')\n",
    "obs_prop.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "obs_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98de4f6-2a92-440b-8565-296900ef48ec",
   "metadata": {},
   "source": [
    "Examine the following histogram of propensity scores, grouped by dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c603a83-6ae9-45df-8984-3c0470574ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(obs_prop[obs_prop['treat'] == 1]['pscore']);\n",
    "plt.title(\"Propensity score of people receiving the treatment (NSW)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf3f5f-b771-4053-9d5c-87ca3ce488dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(obs_prop[obs_prop['treat'] == 0]['pscore']);\n",
    "plt.title(\"Propensity score of people not receiving the treatment (CPS)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0044dfe0-8e3d-46f3-bfc4-90548aedd1bd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 5.a) Explain why the two histograms are so different\n",
    "\n",
    "As you craft your response, be sure to consider the characteristics of the people participating in the training program (see Problem Setup and Data section)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba971d3",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "enter answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f05510c-29ac-48de-9a5b-e0cce3f8b957",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 5.b) Inverse propensity weighting\n",
    "\n",
    "\n",
    "We could use the propensity scores for a number of things, including matching, but in this lab we'll focus on inverse propensity weighting (IPW). Let $n = n_0 + n_1$ be the total number of observations. Recall from lecture that the IPW estimator of the ATE is:\n",
    "\n",
    "$$\n",
    "\\hat{\\tau}_{IPW} =\n",
    "    \\frac{1}{n}\n",
    "    \\underbrace{%\n",
    "         \\sum_{i: Z_i = 1} \\frac{Y_i}{e(X_i)}\n",
    "    }_{\\text{reweighted treated rows}}\n",
    "    -\n",
    "    \\frac{1}{n}\n",
    "    \\underbrace{%\n",
    "        \\sum_{i: Z_i = 0} \\frac{Y_i}{1-e(X_i)}\n",
    "    }_{\\text{reweighted control rows}}\n",
    "$$\n",
    "\n",
    "Note that the weights are different for the two groups. Intuitively, the weights decrease the importance of points that have a high probability of being in the group that they're in.\n",
    "\n",
    "For example, consider two individuals from the CPS data: person A, who looks very different from the treatment (NSW) population, and person B, who looks much more similar to the treatment (NSW) population. Person A's propensity score will be close to 0, so the denominator $1-e(X_A)$ will be close to 1, leaving the value roughly unchanged. Person B's propensity will be closer to 1, so the denominator will be small. This value will then be strongly upweighted. The idea is that we give more weight to control units who look like the treatment group and less weight to control units who don't so that we can make the two groups more comparable. \n",
    "\n",
    "Complete the cell below to compute the IPW estimate for the ATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dc3930-3207-4947-89c6-a0fb4d0dd98c",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "ipw_estimate = ...\n",
    "ipw_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a682ae",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96415060-7372-46f6-9cd7-fee7087fd28f",
   "metadata": {},
   "source": [
    "If you compare the estimate to the truth, you'll notice that **our answer doesn't even believe the effect of the program is positive!** This is a very important lesson: you have to be very careful when doing observational causal inference. Even with a sophistcated propensity score model, it is easy to go wrong. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d946c-aa5e-47d3-8ae4-31dc7ddf4161",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 5.c) Trimming our propensities\n",
    "\n",
    "Why was our result with IPW a negative number? This happens because if some observations are rare in the treatment group (i.e. $e(X) \\approx 0$) then the inverse propensity score, $1/e(X)$ can be enormous. Our estimator gets swamped by variance. Recent work in IPW suggests that a good rule of thumb is to only include points with propensity scores between 0.1 and 0.9 which accepts some bias to reduce the variance.\n",
    "\n",
    "In the cell below, remove any data points with propensity scores that are too low or too high, and repeat the computation in 3e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdc9257-d750-4c92-b46e-49e9c530ac69",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "trimmed_ipw_estimate = ...\n",
    "trimmed_ipw_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7e0f7b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff868ec-b435-4ef7-ac7b-e33647e4336f",
   "metadata": {},
   "source": [
    "This estimate is much closer to the true causal effect we obtained from the randomized experiment in part I, even if it is slightly too large. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd52d66-35bb-4078-8361-97ac88eac96a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 5.d) Interpreting the result\n",
    "\n",
    "Now let's interpret the result of IPW. Fill in the blanks below with the appropriate phrases:\n",
    "\n",
    "*If we assume that ___, then the estimated effect of the program using IPW is that the program causes people to earn ___* *more than they would have.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab7a509",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "enter answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca131e19-8dd3-4732-8721-6ed0c3028ff9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 5e\n",
    "\n",
    "Give at least one reason why the IPW estimate doesnâ€™t exactly match the true estimate, using what you know about the assumptons weâ€™ve made. \n",
    "\n",
    "*Hint: there is more than one right answer.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d09011e",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "enter answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f437ea27-9628-4598-8bb9-45e125d882bc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Congratulations! You have finished Lab 9!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c36d146-323a-4526-a445-07945baa917d",
   "metadata": {},
   "source": [
    "You've reached the end of the lab!\n",
    "\n",
    "Before you submit to Gradescope, make sure you pass all the autograded portions of this lab. **Run the cell below to generate a PDF of your lab submission**, and **run the last cell to generate a zip file of your lab submission.** Do **not** create your lab PDF by exporting your notebook to a PDF.\n",
    "\n",
    "To submit your lab to Gradescope, submit the PDF to Lab 9 Written and the zip file to Lab 9 Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9a34a7-1c48-474e-819d-e033410f2d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "from otter.export import export_notebook\n",
    "from os import path\n",
    "from IPython.display import display, HTML\n",
    "export_notebook(\"lab09.ipynb\", filtering=True, pagebreaks=True)\n",
    "if(path.exists('lab09.pdf')):\n",
    "    img = mpimg.imread('qt.jpg')\n",
    "    imgplot = plt.imshow(img)\n",
    "    imgplot.axes.get_xaxis().set_visible(False)\n",
    "    imgplot.axes.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "    display(HTML(\"Download your PDF <a href='lab09.pdf' download>here</a>.\"))\n",
    "else:\n",
    "    print(\"\\n Pdf generation fails, please try the other methods described above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e62d5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0db784",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eed0e2a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d39808",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.export(pdf=False, force_save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160cab5f",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1b": {
     "name": "q1b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> check = np.round(causal_effect_nsw, 4) == 1805.7953\n>>> assert check\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.round(sdo, 4) == 3423.7105\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> params = linear_model.params\n>>> assert len(params) == 2\n>>> assert np.round(params['treat'], 4) == 3499.0675\n>>> assert np.round(params['educ'], 4) == 72.9273\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3c": {
     "name": "q3c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> params = full_linear_model.params\n>>> assert len(params) == 7\n>>> assert np.round(params['treat'], 4) == 1541.3961\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3g": {
     "name": "q3g",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> boot = draw_tau_hat(obs, 102)\n>>> assert np.round(boot, 4) == 2076.499\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> boots = get_bootstrapped_ate(n=100)\n>>> assert len(boots) == 100\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5b": {
     "name": "q5b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.round(ipw_estimate, 2) == -271.25\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5c": {
     "name": "q5c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.round(trimmed_ipw_estimate, 2) == 1529.69\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
